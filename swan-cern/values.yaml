global:
  # These are placeholders for the LCG values defined in the GitOps repository
  generic:
    lcg: 
    platform: 
  cuda:
    lcg: 
    platform: 
  nxcals:
    lcg: 
    platform:

enabled: &swanCernEnabled true

swan:
  enabled: *swanCernEnabled
  cvmfs-csi:
    extraConfigMaps:
      cvmfs-csi-default-local:
        default.local: |
          CVMFS_HTTP_PROXY="http://ca-proxy.cern.ch:3128"
          CVMFS_QUOTA_LIMIT=20000
          CVMFS_CACHE_BASE=/cvmfs-localcache
    automountDaemonUnmountTimeout: 1800
    cvmfs-csi-config-d:
      sft.cern.ch.conf: |
        CVMFS_HTTP_PROXY='http://ca-proxy-sft.cern.ch:3128;http://ca-proxy.cern.ch:3128'
    nodeplugin:
      prefetcher:
        enabled: true
        image:
          repository: gitlab-registry.cern.ch/swan/docker-images/jupyter/prefetcher
          tag: v0.0.2
        # Jobs to warmup ROOT, NXCALS and CUDA stacks respectively
        jobs:
          - name: cron-warmup-lcg-releases
            schedule: "*/15 * * * *"
            script: |-
              #!/bin/bash

              source /cvmfs/sft.cern.ch/lcg/views/{{ .Values.global.generic.lcg }}/{{ .Values.global.generic.platform }}/setup.sh &&
              (timeout 20s python3 -m ipykernel > /dev/null 2>&1 || true)

              source /cvmfs/sft.cern.ch/lcg/views/{{ .Values.global.generic.lcg }}/{{ .Values.global.generic.platform }}/setup.sh &&
              (timeout 20s python3 -m JupyROOT.kernel.rootkernel > /dev/null 2>&1 || true)

              source /cvmfs/sft.cern.ch/lcg/views/{{ .Values.global.nxcals.lcg }}/{{ .Values.global.nxcals.platform }}/setup.sh &&
              (timeout 20s python3 -m ipykernel > /dev/null 2>&1 || true) &&
              (timeout 20s python3 -c 'import pyspark' || true)

              (lsmod | grep nvidia) &&
              source /cvmfs/sft.cern.ch/lcg/views/{{ .Values.global.cuda.lcg }}/{{ .Values.global.cuda.platform }}/setup.sh &&
              (timeout 20s python3 -c 'import tensorflow' || true) &&
              (timeout 20s python3 -c 'import torch' || true)
  eosxd-csi:
    automountDaemonUnmountTimeout: 1800
  jupyterhub:
    singleuser:
      cpu:
        guarantee: 0.5
      image:
        name: gitlab-registry.cern.ch/swan/docker-images/jupyter/swan-cern
        tag: v0.0.28
      networkPolicy:
        ingress:
          # Allow user pods to receive incoming connections on the NodePort service port range.
          # This supports use cases where servers run inside the user pod and need to receive
          # connections (e.g. Spark, HTCondor via Dask).
          - from:
            - ipBlock:
                cidr: 0.0.0.0/0
            ports:
            - protocol: TCP
              port: 30000
              endPort: 32767
        egress:
          # Allow connection from the user pod to the hadoop token generator to get Hadoop credentials.
          - to:
              - podSelector:
                  matchLabels:
                    app: hadoop-token-generator
            ports:
              - port: 80
          # Allow outgoing connections to the internet, but restrict access to pod network
          # and metadata server.
          - to:
            - ipBlock:
                cidr: 0.0.0.0/0
                except:
                - 10.100.0.0/16
                - 169.254.169.254/32
    scheduling:
      userPods:
        nodeAffinity:
          matchNodePurpose: ignore
    hub:
      extraVolumeMounts:
        - name: swan-jh-cern
          mountPath: /srv/jupyterhub/options_form_config.json
          subPath: options_form_config.json
        - name: swan-jh
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/1_swan_config.py
          subPath: swan_config.py
        - name: swan-jh-cern
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/2_swan_config_cern.py
          subPath: swan_config_cern.py
        - name: swan-jh-cern
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/3_swan_computing_config.py
          subPath: swan_computing_config.py
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/eos.cred
          subPath: eos.cred
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/hadoop.cred
          subPath: hadoop.cred
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/sparkk8s.cred
          subPath: sparkk8s.cred
        - name: cvmfs
          mountPath: /cvmfs
          mountPropagation: HostToContainer
      extraVolumes:
        - name: cvmfs
          persistentVolumeClaim:
            claimName: cvmfs
        - name: swan-jh
          configMap:
            name: swan-scripts
            items:
            - key: swan_config.py
              path: swan_config.py
        - name: swan-jh-cern
          configMap:
            name: swan-scripts-cern
            items:
            - key: options_form_config.json
              path: options_form_config.json
            - key: swan_config_cern.py
              path: swan_config_cern.py
            - key: swan_computing_config.py
              path: swan_computing_config.py
        - name: swan-secrets
          secret:
            secretName: swan-cern
            items:
            - key: eos.cred
              path: eos.cred
            - key: hadoop.cred
              path: hadoop.cred
            - key: sparkk8s.cred
              path: sparkk8s.cred
      config:
        KeyCloakAuthenticator:
          oidc_issuer: https://auth.cern.ch/auth/realms/cern
          admin_role: swan-admins
          exchange_tokens:
            - eos-service
            - cernbox-service
          logout_redirect_url: https://cern.ch/swan
          auto_login: False
          username_claim: preferred_username
          client_id: # placeholder, check secrets
          client_secret: # placeholder, check secrets
          oauth_callback_url: # placeholder, check secrets
          # skip refreshing tokens if already refreshed in last 110 minutes
          # this assumes tokens provided by keycloak are valid for 120 minutes
          auth_refresh_age: 6600
          refresh_pre_spawn: False
        JupyterHub:
          allow_named_servers: False
        SwanKubeSpawner:
          # memory request for user pod (fraction of the limit)
          mem_request_fraction: 0.6
      extraConfig:
        00-authConf: |
          def pre_spawn_hook(authenticator, spawner, auth_state):
            spawner.environment['ACCESS_TOKEN'] = auth_state['exchanged_tokens']['eos-service']
            spawner.environment['OAUTH_INSPECTION_ENDPOINT'] = authenticator.userdata_url.replace('https://', '')
            spawner.user_uid = str(str(auth_state['oauth_user']['cern_uid'])) # k8s only supports values as strings!
            decoded_token = authenticator._decode_token(auth_state['access_token'])
            spawner.user_roles = authenticator.claim_roles_key(authenticator, decoded_token)
          c.KeyCloakAuthenticator.pre_spawn_hook = pre_spawn_hook
        02-spawnError: |
          SPAWN_ERROR_MESSAGE = """SWAN could not start a session for your user, please try again. If the problem persists, please check:
          <ul>
              <li>Do you have a CERNBox account? If not, click <a href="https://cernbox.cern.ch" target="_blank">here</a>.</li>
              <li>If you requested a GPU, are all of them busy? Please ask <a href="https://cern.service-now.com/service-portal?id=functional_element&name=swan" target="_blank">SWAN Support</a>.</li>
              <li>Is there a problem with the service? Find information <a href="https://cern.service-now.com/service-portal?id=service_status_board" target="_blank">here</a>.</li>
              <li>If none of the options apply, please open a <a href="https://cern.service-now.com/service-portal?id=functional_element&name=swan" target="_blank">Support Ticket</a>.</li>
          </ul>"""

          # SWAN@CERN error message
          c.SpawnHandlersConfigs.spawn_error_message = SPAWN_ERROR_MESSAGE
      db:
        type: postgres
        # placeholder for postgres connection url
        url:
        # placeholder for postgres password
        password:
      services:
        hadoop-token-generator: {} # apiToken is generated by the chart
        prometheus-service-monitor: {} # apiToken is generated by the chart
      loadRoles:
        prometheus:
          description: Access to hub Prometheus metrics
          scopes: ['read:metrics']
          services: [prometheus-service-monitor]
      networkPolicy:
        ingress:
          # Allow connection from prometheus to the hub, to scrape the metrics endpoint
          - ports:
              - protocol: TCP
                port: 8081
            from:
              - podSelector:
                  matchLabels:
                    app.kubernetes.io/name: prometheus
                namespaceSelector:
                  matchLabels:
                    kubernetes.io/metadata.name: kube-system
    custom:
      cull:
        # 4 hours
        timeout: 14400
        checkEosAuth: true
        hooksDir: /srv/jupyterhub/culler
      spark:
        configurationPath: /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext
swanCern:
  secrets:
    eos:
      cred:
    hadoop:
      cred:
    sparkk8s:
      cred:

hadoopTokenGenerator:
  image: gitlab-registry.cern.ch/swan/docker-images/hadoop-token-generator:v3.0.4
