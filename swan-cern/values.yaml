swan:
  cvmfs:
    deployDaemonSet: &cvmfsDeployDS true
    deployCsiDriver: &cvmfsDeployCSI false
    useCsiDriver: &cvmfsUseCSI false
    prefetcher:
      enabled: true
      jobs:
        # ROOT
        cron_opennotebook_root_kernel:
          command: >-
            source /cvmfs/sft.cern.ch/lcg/views/LCG_103swan/x86_64-centos7-gcc11-opt/setup.sh &&
            (timeout 20s python3 -m JupyROOT.kernel.rootkernel > /dev/null 2>&1 || true)
          minute: '*/15'
        # NXCALS
        cron_opennotebook_nxcals:
          command: >-
            source /cvmfs/sft.cern.ch/lcg/views/LCG_102b_nxcals_pro/x86_64-centos7-gcc11-opt/setup.sh &&
            (timeout 20s python3 -m ipykernel > /dev/null 2>&1 || true) &&
            (timeout 20s python3 -c 'import pyspark' || true)
          minute: '*/15'
        # CUDA
        cron_opennotebook_cuda:
          command: >-
            (lsmod | grep nvidia) &&
            source /cvmfs/sft.cern.ch/lcg/views/LCG_103cuda/x86_64-centos7-gcc11-opt/setup.sh &&
            (timeout 20s python3 -c 'import tensorflow' || true) &&
            (timeout 20s python3 -c 'import torch' || true)
          minute: '*/15'
    repositories:
      - cvmfs-config.cern.ch
      - sft.cern.ch
      - sft-nightlies.cern.ch
      - alice.cern.ch
      - alice-ocdb.cern.ch
      - alice-nightlies.cern.ch
      - alpha.cern.ch
      - ams.cern.ch
      - atlas.cern.ch
      - atlas-condb.cern.ch
      - atlas-nightlies.cern.ch
      - clicbp.cern.ch
      - cms.cern.ch
      - cms-ib.cern.ch
      - cms-bril.cern.ch
      - compass.cern.ch
      - compass-condb.cern.ch
      - fcc.cern.ch
      - ganga.cern.ch
      - geant4.cern.ch
      - lhcb.cern.ch
      - lhcb-condb.cern.ch
      - lhcbdev.cern.ch
      - na61.cern.ch
      - na62.cern.ch
      - projects.cern.ch
      - sw.hsf.org
      - sndlhc.cern.ch
    resources:
      requests:
        memory: 1.5G
  eos:
    deployDaemonSet: &eosDeployDS false
    deployCsiDriver: &eosDeployCSI true
    useCsiDriver: &eosUseCSI true
  eosxd:
    resources:
      requests:
        memory: 1.5G
  jupyterhub:
    singleuser:
      cpu:
        guarantee: 1
      extraEnv:
        # Enable HTCondor service configuration for CERN in the user image
        CERN_HTCONDOR: "true"   
    scheduling:
      userPods:
        nodeAffinity:
          matchNodePurpose: ignore
    hub:
      extraVolumeMounts:
        - name: swan-jh-cern
          mountPath: /srv/jupyterhub/options_form_config.json
          subPath: options_form_config.json
        - name: swan-jh
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/1_swan_config.py
          subPath: swan_config.py
        - name: swan-jh-cern
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/2_swan_config_cern.py
          subPath: swan_config_cern.py
        - name: swan-jh-cern
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/3_swan_spark_config.py
          subPath: swan_spark_config.py
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/eos.cred
          subPath: eos.cred
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/hadoop.cred
          subPath: hadoop.cred
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/sparkk8s.cred
          subPath: sparkk8s.cred
        - name: cvmfs
          mountPath: /cvmfs
      extraVolumes:
        - name: cvmfs
          hostPath:
            path: /var/cvmfs
            type: Directory
        - name: swan-jh
          configMap:
            name: swan-scripts
            items:
            - key: swan_config.py
              path: swan_config.py
        - name: swan-jh-cern
          configMap:
            name: swan-scripts-cern
            items:
            - key: options_form_config.json
              path: options_form_config.json
            - key: swan_config_cern.py
              path: swan_config_cern.py
            - key: swan_spark_config.py
              path: swan_spark_config.py
        - name: swan-secrets
          secret:
            secretName: swan-cern
            items:
            - key: eos.cred
              path: eos.cred
            - key: hadoop.cred
              path: hadoop.cred
            - key: sparkk8s.cred
              path: sparkk8s.cred
      config:
        KeyCloakAuthenticator:
          oidc_issuer: https://auth.cern.ch/auth/realms/cern
          exchange_tokens:
            - eos-service
            - cernbox-service
          logout_redirect_url: https://cern.ch/swan
          auto_login: False
          username_key: preferred_username
          client_id: # placeholder, check secrets
          client_secret: # placeholder, check secrets
          oauth_callback_url: # placeholder, check secrets

          # skip refreshing tokens if already refreshed in last 110 minutes
          # this assumes tokens provided by keycloak are valid for 120 minutes
          auth_refresh_age: 6600
        JupyterHub:
          allow_named_servers: False
        SwanKubeSpawner:
          # memory request for user pod (fraction of the limit)
          mem_request_fraction: 0.6
      extraConfig:
        00-authConf: |
          def pre_spawn_hook(authenticator, spawner, auth_state):
            spawner.environment['ACCESS_TOKEN'] = auth_state['exchanged_tokens']['eos-service']
            spawner.environment['OAUTH_INSPECTION_ENDPOINT'] = authenticator.userdata_url.replace('https://', '')
            spawner.user_uid = str(str(auth_state['oauth_user']['cern_uid'])) # k8s only supports values as strings!
            decoded_token = authenticator._decode_token(auth_state['access_token'])
            spawner.user_roles = authenticator.claim_roles_key(authenticator, decoded_token)
          c.KeyCloakAuthenticator.pre_spawn_hook = pre_spawn_hook
        02-spawnError: |
          SPAWN_ERROR_MESSAGE = """SWAN could not start a session for your user, please try again. If the problem persists, please check:
          <ul>
              <li>Do you have a CERNBox account? If not, click <a href="https://cernbox.cern.ch" target="_blank">here</a>.</li>
              <li>If you requested a GPU, are all of them busy? Please ask <a href="https://cern.service-now.com/service-portal?id=functional_element&name=swan" target="_blank">SWAN Support</a>.</li>
              <li>Is there a problem with the service? Find information <a href="https://cern.service-now.com/service-portal?id=service_status_board" target="_blank">here</a>.</li>
              <li>If none of the options apply, please open a <a href="https://cern.service-now.com/service-portal?id=functional_element&name=swan" target="_blank">Support Ticket</a>.</li>
          </ul>"""

          # SWAN@CERN error message
          c.SpawnHandlersConfigs.spawn_error_message = SPAWN_ERROR_MESSAGE
      db:
        type: postgres
        # placeholder for postgres connection url
        url:
        # placeholder for postgres password
        password:
      services:
        hadoop-token-generator: {} # apiToken is generated by the chart
        prometheus-service-monitor: {} # apiToken is generated by the chart
          
    custom:
      cull:
        # 4 hours
        timeout: 14400
        checkEosAuth: true
        hooksDir: /srv/jupyterhub/culler
      cvmfs:
        deployDaemonSet: *cvmfsDeployDS
        deployCsiDriver: *cvmfsDeployCSI
        useCsiDriver: *cvmfsUseCSI
        repositories:
          - mount: cvmfs-config.cern.ch
          - mount: sft.cern.ch
            proxy: 'http://ca-proxy-sft.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: sft-nightlies.cern.ch
            proxy: 'http://ca-proxy-sft.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alice.cern.ch
            proxy: 'http://ca-proxy-alice.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alice-ocdb.cern.ch
            proxy: 'http://ca-proxy-alice.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alice-nightlies.cern.ch
            proxy: 'http://ca-proxy-alice.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alpha.cern.ch
          - mount: ams.cern.ch
          - mount: atlas.cern.ch
            proxy: 'http://ca-proxy-atlas.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: atlas-condb.cern.ch
            proxy: 'http://ca-proxy-atlas.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: atlas-nightlies.cern.ch
            proxy: 'http://ca-proxy-atlas.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: clicbp.cern.ch
          - mount: cms.cern.ch
            proxy: 'http://cmsmeyproxy.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: cms-ib.cern.ch
            proxy: 'http://cmsmeyproxy.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: cms-bril.cern.ch
            proxy: 'http://cmsmeyproxy.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: compass.cern.ch
            proxy: 'http://ca-proxy-compass.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: compass-condb.cern.ch
            proxy: 'http://ca-proxy-compass.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: fcc.cern.ch
          - mount: ganga.cern.ch
          - mount: geant4.cern.ch
          - mount: lhcb.cern.ch
            proxy: 'http://ca-proxy-lhcb.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: lhcb-condb.cern.ch
            proxy: 'http://ca-proxy-lhcb.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: lhcbdev.cern.ch
            proxy: 'http://ca-proxy-lhcb.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: na61.cern.ch
          - mount: na62.cern.ch
          - mount: projects.cern.ch
          - mount: sw.hsf.org
      eos:
        deployDaemonSet: *eosDeployDS
        deployCsiDriver: *eosDeployCSI
        useCsiDriver: *eosUseCSI
      spark:
        configurationPath: /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext
swanCern:
  secrets:
    eos:
      cred:
    hadoop:
      cred:
    sparkk8s:
      cred:

hadoopTokenGenerator:
  image: gitlab-registry.cern.ch/swan/docker-images/hadoop-token-generator:v2.1.1

fluentd:
  containerRuntime: containerd
  plugins:
    - fluent-plugin-rewrite-tag-filter
    - fluent-plugin-out-http
    - fluent-plugin-grok-parser
  output:
    endpoint: http://monit-logs.cern.ch:10012/
    includeInternal: false
  parsingConfig: |
    # Initial tagging of logs:
    # - user: user session pods
    # - hub: hub pod
    # - logs: other pods
    <match kubernetes.**>
        @type rewrite_tag_filter
        capitalize_regex_backreference true
        <rule>
            key $.kubernetes.labels.component
            pattern /^singleuser-server$/
            tag user
        </rule>
        <rule>
            key $.kubernetes.labels.component
            pattern /^hub$/
            tag hub
        </rule>
        <rule>
            key $.kubernetes.pod_name
            pattern /.?/
            tag logs 
        </rule>
    </match>

    # Retag user and hub logs to:
    # - metrics: jupyter and jupyterhub custom log messages, used to emit metrics
    # - logs: rest of log messages
    <match {user,hub}>
        @type rewrite_tag_filter
        capitalize_regex_backreference true
        <rule>
            key $.log
            pattern /^.*?user: .*?, host: .*?, metric: .*?, value: .*$/
            tag metrics
        </rule>
        <rule>
            key $.log
            pattern /.?/
            tag logs
        </rule>
    </match>

    # Extract relevant information from metric logs:
    # - user
    # - host
    # - metric key: name of the metric
    # - metric context (optional): additional qualifiers for the metric
    # - metric value
    <filter {metrics}>
        @type parser
        key_name log

        reserve_data true
        emit_invalid_record_to_error true

        <parse>
            @type grok

            # keep unmatched logs
            grok_failure_key grokfailure

            <grok>
                # messages printed with "metric: metric_key.metric_context", where metric_context can have dots
                pattern user: %{DATA:_metric_user_}, host: %{DATA}, metric: (?<_metric_key_>(\w*))(\.)(?<_metric_context_>(.*)), value: %{GREEDYDATA:_metric_value_}
            </grok>
            <grok>
                # messages printed with "metric: metric_key"
                pattern user: %{DATA:_metric_user_}, host: %{DATA}, metric: (?<_metric_key_>(.*))(?<_metric_context_>(.*)), value: %{GREEDYDATA:_metric_value_}
            </grok>
        </parse>
    </filter>

    # Store extracted metric information in a new "metrics" field, as part of the JSON
    # of the log message. Use metric key as first attribute under "metrics".
    # Format: metrics.metric_key.user|context|value
    <filter {metrics}>
        @type record_transformer
        enable_ruby
        <record>
            metrics.${record['_metric_key_']}.user ${record['_metric_user_']}
            metrics.${record['_metric_key_']}.context ${record['_metric_context_']}
            metrics.${record['_metric_key_']}.value ${record['_metric_value_']}

            # the below field is needed to run queries that group values by the last segment of the metric context (segments split by '.')
            # which looks like "metrics.{record['_metric_key_']}_lastelementofmetric_context = {record['metric_value']}"
            # for example, for a record with below fields
            #     "data.metrics.spawn.context = LCG_xxx.some_spark_cluster.exception_class"
            #     "data.metrics.spawn.value = None"
            # it emits a field "data.metrics.spawn_exception_class = None
            metrics.${record['_metric_key_']}_${record['_metric_context_'].gsub('-','_').split('.')[-1]} ${record['_metric_value_']}
        </record>
        remove_keys ["_metric_user_", "_metric_key_", "_metric_context_", "_metric_value_"]
    </filter>

    # Emit general log records with type "logs"
    # Accessible in OpenSearch with pattern monit_private_swan_logs_logs
    <filter {logs}>
        @type record_transformer
        <record>
            type "logs"
        </record>
    </filter>

    # Emit metric log records with type "metrics"
    # Accessible in OpenSearch with pattern monit_private_swan_logs_metrics
    <filter {metrics}>
        @type record_transformer
        <record>
            type "metrics"
        </record>
    </filter>

    # Set MONIT producer for all logs
    # Keep raw log data in "raw" field
    <filter {logs,metrics}>
        @type record_transformer
        enable_ruby
        <record>
            timestamp ${(time.to_f * 1000).to_i}
            producer "#{ENV['OUTPUT_PRODUCER']}"
            raw ${record["log"]} # field named raw is indexed by MONIT/OpenSearch
        </record>
        remove_keys ["log"]
    </filter>

    # Push logs to MONIT endpoint
    <match {logs,metrics}>
        @type http
        endpoint_url    "#{ENV['OUTPUT_ENDPOINT']}"
        serializer      json
        http_method     post
    </match>

    # Forward errors in this pipeline to fluentd pod stdout for debugging
    <label @ERROR>
        <match **>
            @type stdout
        </match>
    </label>
