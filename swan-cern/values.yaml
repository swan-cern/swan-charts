swan:
  cvmfs:
    deployDaemonSet: &cvmfsDeployDS true
    deployCsiDriver: &cvmfsDeployCSI false
    useCsiDriver: &cvmfsUseCSI false
    prefetcher:
      enabled: true
      jobs:
        cron_opennotebook_python2_ipy:
          command: "source /cvmfs/sft.cern.ch/lcg/views/LCG_99python2/x86_64-centos7-gcc8-opt/setup.sh && ( timeout 20s python -m ipykernel > /dev/null 2>&1 || true )"
          minute: '*/15'
        cron_opennotebook_python2_root:
          command: "source /cvmfs/sft.cern.ch/lcg/views/LCG_99python2/x86_64-centos7-gcc8-opt/setup.sh && ( timeout 20s python -m JupyROOT.kernel.rootkernel > /dev/null 2>&1 || true )"
          minute: '*/15'
        cron_opennotebook_python3_root:
          command: "source /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/setup.sh && ( timeout 20s python -m JupyROOT.kernel.rootkernel > /dev/null 2>&1 || true )"
          minute: '*/15'
        cron_opennotebook_python3nx_ipy:
          command: "source /cvmfs/sft.cern.ch/lcg/views/LCG_95apython3_nxcals/x86_64-centos7-gcc7-opt/setup.sh && ( timeout 20s python -m ipykernel > /dev/null 2>&1 || true )"
          minute: '*/15'
        cron_opennotebook_python3nx_spark:
          command: "source /cvmfs/sft.cern.ch/lcg/views/LCG_100_nxcals/x86_64-centos7-gcc9-opt/setup.sh && ( timeout 20s python -c 'import pyspark' > /dev/null 2>&1 || true )"
          minute: '*/15'
        cron_opennotebook_cuda_tensorflow:
          command: "(lsmod | grep nvidia) && source /cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/setup.sh && ( timeout 60s python -c 'import tensorflow' > /dev/null 2>&1 || true )"
          minute: '5,20,35,50'
        cron_opennoteook_cuda_torch:
          command: "(lsmod | grep nvidia) && source /cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/setup.sh && ( timeout 60s python -c 'import torch' > /dev/null 2>&1 || true )"
          minute: '10,25,40,55'
  eos:
    deployDaemonSet: &eosDeployDS false
    deployCsiDriver: &eosDeployCSI true
    useCsiDriver: &eosUseCSI true
  jupyterhub:
    hub:
      extraVolumeMounts:
        - name: swan-jh-cern
          mountPath: /srv/jupyterhub/options_form_config.json
          subPath: options_form_config.json
        - name: swan-jh
          mountPath: /srv/jupyterhub/jupyterhub_form.html
          subPath: jupyterhub_form.html
        - name: swan-jh
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/1_swan_config.py
          subPath: swan_config.py
        - name: swan-jh-cern
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/2_swan_config_cern.py
          subPath: swan_config_cern.py
        - name: swan-jh-cern
          mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/3_swan_spark_config.py
          subPath: swan_spark_config.py
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/eos.cred
          subPath: eos.cred
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/hadoop.cred
          subPath: hadoop.cred
        - name: swan-secrets
          mountPath: /srv/jupyterhub/private/sparkk8s.cred
          subPath: sparkk8s.cred
        - name: swan-cull-scripts
          mountPath: /srv/jupyterhub/culler/check_ticket.sh
          subPath: cull_check_ticket.sh
        - name: swan-cull-scripts
          mountPath: /srv/jupyterhub/culler/delete_ticket.sh
          subPath: cull_delete_ticket.sh
        - name: swan-tokens-scripts
          mountPath: /srv/jupyterhub/private/eos_token.sh
          subPath: eos_token.sh
        - name: swan-tokens-scripts
          mountPath: /srv/jupyterhub/private/webhdfs_token.sh
          subPath: webhdfs_token.sh
        - name: swan-tokens-scripts
          mountPath: /srv/jupyterhub/private/sparkk8s_token.sh
          subPath: sparkk8s_token.sh
        - name: cvmfs
          mountPath: /cvmfs
      extraVolumes:
        - name: cvmfs
          hostPath:
            path: /var/cvmfs
            type: Directory
        - name: swan-jh
          configMap:
            name: swan-scripts
            items:
            - key: jupyterhub_form.html
              path: jupyterhub_form.html
            - key: swan_config.py
              path: swan_config.py
        - name: swan-jh-cern
          configMap:
            name: swan-scripts-cern
            items:
            - key: options_form_config.json
              path: options_form_config.json
            - key: swan_config_cern.py
              path: swan_config_cern.py
            - key: swan_spark_config.py
              path: swan_spark_config.py
        - name: swan-cull-scripts
          configMap:
            name: swan-scripts-cern
            items:
            - key: cull_check_ticket.sh
              path: cull_check_ticket.sh
            - key: cull_delete_ticket.sh
              path: cull_delete_ticket.sh
            defaultMode: 356 # 0544 perm
        - name: swan-tokens-scripts
          configMap:
            name: swan-scripts-env-prod
            items:
            - key: webhdfs_token.sh
              path: webhdfs_token.sh
            - key: eos_token.sh
              path: eos_token.sh
            - key: sparkk8s_token.sh
              path: sparkk8s_token.sh
            defaultMode: 356 # 0544 perm
        - name: swan-secrets
          secret:
            secretName: swan-cern
            items:
            - key: eos.cred
              path: eos.cred
            - key: hadoop.cred
              path: hadoop.cred
            - key: sparkk8s.cred
              path: sparkk8s.cred
      config:
        KeyCloakAuthenticator:
          oidc_issuer: https://auth.cern.ch/auth/realms/cern
          exchange_tokens:
            - eos-service
            - cernbox-service
          logout_redirect_uri: https://cern.ch/swan
          auto_login: True
          username_key: preferred_username
          client_id: # placeholder, check secrets
          client_secret: # placeholder, check secrets
          oauth_callback_url: # placeholder, check secrets
        JupyterHub:
          allow_named_servers: False
      extraConfig:
        00-authConf: |
          def pre_spawn_hook(authenticator, spawner, auth_state):
            spawner.environment['ACCESS_TOKEN'] = auth_state['exchanged_tokens']['eos-service']
            spawner.environment['OAUTH_INSPECTION_ENDPOINT'] = authenticator.userdata_url.replace('https://', '')
            spawner.user_uid = str(str(auth_state['oauth_user']['cern_uid'])) # k8s only supports values as strings!
            decoded_token = authenticator._decode_token(auth_state['access_token'])
            spawner.user_roles = authenticator.claim_roles_key(authenticator, decoded_token)
          c.KeyCloakAuthenticator.pre_spawn_hook = pre_spawn_hook
        02-spawnError: |
          SPAWN_ERROR_MESSAGE = """SWAN could not start a session for your user, please try again. If the problem persists, please check:
          <ul>
              <li>Do you have a CERNBox account? If not, click <a href="https://cernbox.cern.ch" target="_blank">here</a>.</li>
              <li>Is there a problem with the service? Find information <a href="https://cern.service-now.com/service-portal?id=service_status_board" target="_blank">here</a>.</li>
              <li>If none of the options apply, please open a <a href="https://cern.service-now.com/service-portal?id=functional_element&name=swan" target="_blank">Support Ticket</a>.</li>
          </ul>"""

          # SWAN@CERN error message
          c.SpawnHandlersConfigs.spawn_error_message = SPAWN_ERROR_MESSAGE
      db:
        type: postgres
        # placeholder for postgres connection url
        url:
        # placeholder for postgres password
        password:
      services:
        hadoop-token-generator: {} # apiToken is generated by the chart
          
    custom:
      cull:
        # 6 hours
        timeout: 21600
        checkEosAuth: true
        hooksDir: /srv/jupyterhub/culler
      cvmfs:
        deployDaemonSet: *cvmfsDeployDS
        deployCsiDriver: *cvmfsDeployCSI
        useCsiDriver: *cvmfsUseCSI
        repositories:
          - mount: cvmfs-config.cern.ch
          - mount: sft.cern.ch
            proxy: 'http://ca-proxy-sft.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: sft-nightlies.cern.ch
            proxy: 'http://ca-proxy-sft.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alice.cern.ch
            proxy: 'http://ca-proxy-alice.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alice-ocdb.cern.ch
            proxy: 'http://ca-proxy-alice.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alice-nightlies.cern.ch
            proxy: 'http://ca-proxy-alice.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: alpha.cern.ch
          - mount: ams.cern.ch
          - mount: atlas.cern.ch
            proxy: 'http://ca-proxy-atlas.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: atlas-condb.cern.ch
            proxy: 'http://ca-proxy-atlas.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: atlas-nightlies.cern.ch
            proxy: 'http://ca-proxy-atlas.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: clicbp.cern.ch
          - mount: cms.cern.ch
            proxy: 'http://cmsmeyproxy.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: cms-ib.cern.ch
            proxy: 'http://cmsmeyproxy.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: cms-bril.cern.ch
            proxy: 'http://cmsmeyproxy.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: compass.cern.ch
            proxy: 'http://ca-proxy-compass.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: compass-condb.cern.ch
            proxy: 'http://ca-proxy-compass.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: fcc.cern.ch
          - mount: ganga.cern.ch
          - mount: geant4.cern.ch
          - mount: lhcb.cern.ch
            proxy: 'http://ca-proxy-lhcb.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: lhcb-condb.cern.ch
            proxy: 'http://ca-proxy-lhcb.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: lhcbdev.cern.ch
            proxy: 'http://ca-proxy-lhcb.cern.ch:3128;http://ca-proxy.cern.ch:3128'
          - mount: na61.cern.ch
          - mount: na62.cern.ch
          - mount: projects.cern.ch
          - mount: sw.hsf.org
      eos:
        deployDaemonSet: *eosDeployDS
        deployCsiDriver: *eosDeployCSI
        useCsiDriver: *eosUseCSI
      spark:
        configurationPath: /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext
swanCern:
  secrets:
    eos:
      cred:
    hadoop:
      cred:
    sparkk8s:
      cred:

hadoopTokenGenerator:
  extraVolumes:
    - name: swan-tokens-scripts
      configMap:
        name: swan-scripts-env-prod
        items:
          - key: hadoop_token.sh
            path: hadoop_token.sh
        defaultMode: 500
fluentd:
  plugins:
    - fluent-plugin-rewrite-tag-filter
    - fluent-plugin-out-http
    - fluent-plugin-grok-parser
    - fluent-plugin-concat
  output:
    producer: swan
    endpoint: http://monit-logs.cern.ch:10012/
    includeInternal: false
  parsingConfig: |
    ### Define Classes of logs ###

    <match kubernetes.**>
      @type rewrite_tag_filter
      capitalize_regex_backreference true
      # jupyter notebook logs
      <rule>
        key $.kubernetes.pod_name
        pattern ^(jupyter-|hub-|proxy-|cvmfs-prefetcher-)
        tag swan
      </rule>
      <rule>
        key $.kubernetes.pod_name
        pattern (.?)
        tag system
      </rule>
    </match>
    <match {swan}>
      @type rewrite_tag_filter
      capitalize_regex_backreference true
      # jupyter or jupyterhub custom swan metrics
      <rule>
        key $.log
        pattern (^.*?user: .*?, host: .*?, metric: .*?, value: .*$)
        tag swan.metrics
      </rule>
      <rule>
        key $.log
        pattern (.?)
        tag swan.logs
      </rule>
    </match>

    ### SWAN Jupyter/Jupyterhub logs parsing and output ###

    # jupyter containers parser
    <filter {swan.logs}>
      @type concat
      key log

      # concat long json logs (e.g. swan env debug)
      stream_identity_key container_id
      multiline_end_regexp /\n$/
      separator ""
      timeout_label @swan.logs.multiline
    </filter>
    <filter {swan.logs}>
      @type concat
      key log

      # parse jupyterhub multiline debug log with indent
      stream_identity_key container_id
      multiline_start_regexp /^(\[.*\] pod jupyter-\w+ events before launch)/
      continuous_line_regexp /^(\s)/
      separator ""
      flush_interval 1
      timeout_label @swan.logs.multiline
    </filter>
    <filter {swan.logs}>
      @type concat
      key log

      # parse python multiline error logs with indent
      # handle also timeout waiting for new log lines
      stream_identity_key container_id
      multiline_start_regexp /^(\[E.*\])/
      continuous_line_regexp /^(\s)/
      separator ""
      flush_interval 1
      timeout_label @swan.logs.multiline
    </filter>
    <match {swan.logs}>
      @type relabel
      @label @swan.logs.multiline
    </match>
    <label @swan.logs.multiline>
      # all logs that were parsed with multiline concat filter or got timeout waiting for new logs to parse multiline

      <filter {swan.logs}>
        @type parser
        key_name log

        # keep original record data additionaly to parsed one
        reserve_data true
        emit_invalid_record_to_error true

        # grok parsing
        <parse>
          @type grok

          # keep unmatched logs
          grok_success_key grok_success
          grok_failure_key grok_failure

          <grok>
            # jupyterhub hub message (including multiline)
            pattern \[%{WORD:log_level} %{TIMESTAMP_ISO8601:log_time} %{DATA:log_source}\] (?<log_msg_multiline>(.|\r|\n)*)
            time_format "%Y-%m-%d %H:%M:%S.%N"
            keep_time_key false
            time_key log_time
            timezone +0100
          </grok>

          <grok>
            # jupyterhub culler message (including multiline)
            pattern \[%{WORD:log_level} (?<log_time>%{YEAR}%{MONTHNUM}%{MONTHDAY} %{TIME}) %{DATA:log_source}\] (?<log_msg_multiline>(.|\r|\n)*)
            time_format "%y%m%d %H:%M:%S"
            keep_time_key false
            time_key log_time
            timezone +0100
          </grok>

          <grok>
            # jupyterhub proxy message (including multiline)
            pattern (?<log_time>%{TIME}.%{DATA}) \[%{DATA:log_source}\] (.*?)(?<log_level>(debug|info|error|warn|critical))([^$]*): (?<log_msg_multiline>(.|\r|\n)*)
            time_format "%H:%M:%S.%N"
            keep_time_key false
            time_key log_time
            timezone +0000
          </grok>

          <grok>
            # jupyterhub generic message (including multiline)
            pattern (?<log_msg_multiline>(.|\r|\n)*)
          </grok>
        </parse>
      </filter>
      <filter {swan.logs}>
        @type record_transformer
        enable_ruby
        <record>
          # es wont search in string longer than 256
          log_msg ${if record['log_msg_multiline'].length >= 250; record['log_msg_multiline'][0...250] + "..."; else; record['log_msg_multiline']; end;}
          log_source ${if record['log_source'] != nil; record['log_source']; else; "log"; end;}
          log_level ${if record['log_level'] != nil; record['log_level']; else; "info"; end;}
          timestamp ${(time.to_f * 1000).to_i}

          # define monit producer
          producer "#{ENV['OUTPUT_PRODUCER']}"
          type "swanqa"
        </record>
        remove_keys ["log"]
      </filter>
      <match {swan.logs}>
        @type http
        endpoint_url    "#{ENV['OUTPUT_ENDPOINT']}"
        serializer      json
        http_method     post
      </match>
    </label>

    ### SWAN Metrics logs parsing and output ###

    # parse jupyter and jupyterhub metrics
    <filter {swan.metrics}>
      @type parser
      key_name log

      reserve_data true
      emit_invalid_record_to_error true

      # grok parsing
      <parse>
        @type grok

        # keep unmatched logs
        grok_failure_key grokfailure

        <grok>
          pattern \[%{WORD} %{TIMESTAMP_ISO8601:log_time} %{DATA}\] %{GREEDYDATA:log_msg}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key log_time
          timezone +0100
        </grok>
      </parse>
    </filter>
    <filter {swan.metrics}>
      @type parser
      key_name log_msg

      reserve_data true
      emit_invalid_record_to_error true

      # grok parsing
      <parse>
        @type grok

        # keep unmatched logs
        grok_failure_key grokfailure

        <grok>
          # string type metric_key.metric_context.metric_type
          pattern user: %{DATA:_metric_user_}, host: %{DATA}, metric: (?<_metric_key_>(\w*))(\.)(?<_metric_context_>(.*)), value: %{GREEDYDATA:_metric_value_}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key metric_time
          timezone +0100
        </grok>
        <grok>
          # string type metric_key
          pattern user: %{DATA:_metric_user_}, host: %{DATA}, metric: (?<_metric_key_>(.*))(?<_metric_context_>(.*)), value: %{GREEDYDATA:_metric_value_}
          time_format "%Y-%m-%d %H:%M:%S.%N"
          keep_time_key false
          time_key metric_time
          timezone +0100
        </grok>
      </parse>
    </filter>
    <filter {swan.metrics}>
      @type record_transformer
      enable_ruby
      <record>
        metrics.${record['_metric_key_']}.user ${record['_metric_user_']}
        metrics.${record['_metric_key_']}.context ${record['_metric_context_']}
        metrics.${record['_metric_key_']}.value ${record['_metric_value_']}
        #the gsub is needed to match the indexes in ES, the split [-1] to remove lcg version from exception message
        metric.${record['_metric_key_']}_${record['_metric_context_'].gsub('-','_').split('.')[-1]} ${record['_metric_value_']}
      </record>
      remove_keys ["_metric_time_", "_metric_raw_msg_", "_metric_user_", "_metric_key_", "_metric_context_", "_metric_value_"]
    </filter>
    <filter {swan.metrics}>
      @type record_transformer
      enable_ruby
      <record>
        producer "#{ENV['OUTPUT_PRODUCER']}"
        type "metricsqa"
        timestamp ${(time.to_f * 1000).to_i}
      </record>
      remove_keys ["log"]
    </filter>
    <match {swan.metrics}>
      @type http
      endpoint_url    "#{ENV['OUTPUT_ENDPOINT']}"
      serializer      json
      http_method     post
    </match>

    ### System logs parsing and output ###

    <filter {system}>
      @type record_transformer
      enable_ruby
      <record>
        producer "#{ENV['OUTPUT_PRODUCER']}"
        type "system"
        timestamp ${time.to_i * 1000}
        log_msg ${record['log'].strip}
      </record>
      remove_keys ["log"]
    </filter>
    <match {system}>
      @type http
      endpoint_url    "#{ENV['OUTPUT_ENDPOINT']}"
      serializer      json
      http_method     post
    </match>


    ### Common error handling ###

    # rescue errors to fluentd pod stdout for debugging
    <label @ERROR>
      <match **>
        @type stdout
      </match>
    </label>
