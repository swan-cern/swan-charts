# It seems that starting from our kubernetes templates 1.22 up, the nvidia-device-plugin is crashing
# and the gpu nodes are not setup as expected. There is an issue upstream
# https://github.com/NVIDIA/k8s-device-plugin/issues/253 where a user presents exactly our symptoms.
# This seems to be a race condition of some kind and one way to go around it is by labelling the node
# once with a 'default' mig.config=disabled option so that the migManager pod triggers a restart of the
# crashing pods.

{{- if (index .Values "gpu-operator" "enabled") }}
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-hack-force-mig-config
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: nvidia-hack-force-mig-config
  template:
    metadata:
      labels:
        k8s-app: nvidia-hack-force-mig-config
    spec:
      serviceAccountName: admin
      containers:
      - image: registry.cern.ch/kubernetes/ops:0.1.0
        name: force-mig-config-sleep
        command: ["/bin/sh", "-c", "sleep inf"]
      initContainers:
      # check https://github.com/NVIDIA/k8s-device-plugin/issues/253 for why this is needed
      - image: registry.cern.ch/kubernetes/ops:0.1.0
        name: force-mig-config
        command:
        - /bin/bash
        - "-c"
        - "/usr/local/bin/kubectl -n kube-system label node $NODE_NAME nvidia.com/mig.config=disabled --overwrite"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: kubectl
          mountPath: /usr/local/bin/kubectl
      volumes:
      - name: kubectl
        hostPath:
          path: /usr/local/bin/kubectl
          type: File
      nodeSelector:
        feature.node.kubernetes.io/pci-10de.present: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
{{- end }}
