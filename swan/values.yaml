#
# CVMFS access
# - deployDaemonSet deploys CVMFS pods exposing `/cvmfs` path on the host.
#     Access to CVMFS is provided by bind-mounting `/cvmfs` from the host.
# - deployCsiDriver deploys a cluster-wide storage driver for CVMFS.
#     Access to CVMFS is provided by persistent volume claims.
# - useCsiDriver has to be used in case the hosting infrastructure provides
#       a CSI driver to access CVMFS (i.e., it is not needed to deploy additional pods).
#     Access to CVMFS is provided by persistent volume claims (identical to deployCsiDriver).
# - repositories defines which CVMFS repos have to be mounted into singleusers' pods.
#       The value passed depends on the method used, DaemonSet VS CSI driver.
#
# Defaults best support a stand-alone small deployment:
# - Deploy a DaemonSet pod with the CVMFS client running inside
# - Chunks from upstream are fetched connecting to the CERN Stratum 1 server
#
# Warning:
# - It is discouraged to enable more than one at once.
# - By setting all to false, access to EOS will not be possible
#     and singleuser's session will not be able to start.
#
cvmfs:
  deployDaemonSet: &cvmfsDeployDS true
  deployCsiDriver: &cvmfsDeployCSI false
  useCsiDriver: &cvmfsUseCSI false
  repositories: &cvmfsRepos
    - cvmfs-config.cern.ch
    - sft.cern.ch
    - sft-nightlies.cern.ch
  mountOptions:
    hostMountpoint: /var/cvmfs
  # Prefetcher is provided only by the daemonSet
  prefetcher:
    enabled: true
    jobs:
      # Python3 kernel
      cron_opennotebook_python3_kernel:
        command: >-
          source /cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-centos7-gcc11-opt/setup.sh &&
          (timeout 20s python3 -m ipykernel > /dev/null 2>&1 || true)
        minute: '*/15'

#
# EOS access
# - deployDaemonSet deploys EOS fusex pods exposing `/eos` path on the host.
#     Access to EOS is provided by bind-mounting `/eos` from the host.
# - deployCsiDriver deploys a cluster-wide storage driver for EOS.
#     Access to EOS is provided by persistent volume claims.
# - useCsiDriver has to be used in case the hosting infrastructure provides
#       a CSI driver to access EOS (i.e., it is not needed to deploy additional pods).
#     Access to EOS is provided by persistent volume claims (identical to deployCsiDriver).
#
# Warning:
# - It is discouraged to enable more than one at once.
# - By setting all to false, access to EOS will not be possible.
#     You will need to set `juyterhub.hub.config.SwanKubeSpawner.local_home: true`
#
eos:
  deployDaemonSet: &eosDeployDS true
  deployCsiDriver: &eosDeployCSI false
  useCsiDriver: &eosUseCSI false
fusex:
  fusex:
    hostMountpoint: /var/eos

# Ensure file versions are shown to allow notebook checkpoints
# (the fusex chart already sets this as default)
eosxd:
  config:
    global:
      options:
        hide-versions: 0

#
# JupyterHub
#
jupyterhub:
  singleuser:
    uid: 0
    fsGid: 0
    storage:
      type: none
    image:
      name: "gitlab-registry.cern.ch/swan/docker-images/jupyter/swan-cern"
      tag: "v0.0.8"
      pullPolicy: "Always"
    cloudMetadata:
      # until we configure networkPolicy
      blockWithIptables: true
      ip: 169.254.169.254
    networkPolicy:
      enabled: false
    extraAnnotations:
      kubectl.kubernetes.io/default-container: notebook
    extraEnv:
      SWAN_DISABLE_NOTIFICATIONS: "true"
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 100m
    ingressClassName: nginx
    #tls:
    #  - secretName: swan-tls-cert
    # placeholder for hostname
    hosts:
  proxy:
    service:
      type: ClusterIP
    chp:
      image:
        name: "jupyterhub/configurable-http-proxy"
        tag: "4.5.0"
        pullPolicy: "IfNotPresent"
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
    # placeholder for hub secret token
    secretToken:
  hub:
    podSecurityContext:
      fsGroup: 0
    containerSecurityContext:
      runAsUser: 0
      runAsGroup: 0
    deploymentStrategy:
      type: RollingUpdate
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
    livenessProbe:
      enabled: true
      initialDelaySeconds: 300
      periodSeconds: 15
      failureThreshold: 20
      timeoutSeconds: 10
    readinessProbe:
      enabled: false
    image:
      name: "gitlab-registry.cern.ch/swan/docker-images/jupyterhub"
      tag: "v3.10"
      pullPolicy: "Always"
    extraVolumeMounts:
      - name: swan-jh
        mountPath: /srv/jupyterhub/options_form_config.json
        subPath: options_form_config.json
      - name: swan-jh
        mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/1_swan_config.py
        subPath: swan_config.py
    extraVolumes:
      - name: swan-jh
        configMap:
          name: swan-scripts
          items:
          - key: options_form_config.json
            path: options_form_config.json
          - key: swan_config.py
            path: swan_config.py
    config:
      KeyCloakAuthenticator:
        # Config missing
        oidc_issuer:
        scope:
          - profile
          - email
          - offline_access
          - openid
        exchange_tokens: []
        auto_login: True
        allow_all: True
        client_id: # placeholder, check secrets
        client_secret: # placeholder, check secrets
        oauth_callback_url: # placeholder, check secrets
      SwanSpawner:
        options_form_config: /srv/jupyterhub/options_form_config.json
        # Give notebook 45s to start a webserver and max 60s for whole spawn process
        http_timeout: 45
        start_timeout: 60
        consecutive_failure_limit: 0
      KubeSpawner:
        delete_pvc: False
      SwanKubeSpawner:
        # set home directory to EOS
        local_home: False
        centos7_image: "gitlab-registry.cern.ch/swan/docker-images/systemuser:v6.0.0"
      SpawnHandlersConfigs:
        # disable some defaults of swanspawner that do now work for kube-spawner
        # FIXME remove this from the spawner once we support only k8s
        metrics_on: False
        local_home: True
      JupyterHub:
        authenticator_class: keycloakauthenticator.KeyCloakAuthenticator
        spawner_class: swanspawner.SwanKubeSpawner
        cleanup_servers: False
        tornado_settings:
          # currently we customize spawnhandler to stay in form before redirecting the user, as upstream does
          # FIXME remove once we remove the the metrics from the spawn
          slow_spawn_timeout: 15
        allow_named_servers: False
    extraConfig:
      00-authConf: |
        def pre_spawn_hook(authenticator, spawner, auth_state):
          raise Exception("Please configure pre_spawn_hook")
        c.KeyCloakAuthenticator.pre_spawn_hook = pre_spawn_hook
    networkPolicy:
      enabled: false
    # placeholder for hub cookieSecret
    # when empty, it generates a new randomly
    cookieSecret:
  scheduling:
    userScheduler:
      enabled: false
    podPriority:
      enabled: false
  prePuller:
    hook:
      enabled: true
    continuous:
      enabled: false
    containerSecurityContext:
      allowPrivilegeEscalation: true
  # disable upstream cull, but enable custom one
  cull:
    enabled: false
  custom:
    cull:
      enabled: true
      every: 600
      # 2 hours
      timeout: 7200
      users: true
      checkEosAuth: false
    cvmfs:
      deployDaemonSet: *cvmfsDeployDS
      deployCsiDriver: *cvmfsDeployCSI
      useCsiDriver: *cvmfsUseCSI
      repositories: *cvmfsRepos
    eos:
      deployDaemonSet: *eosDeployDS
      deployCsiDriver: *eosDeployCSI
      useCsiDriver: *eosUseCSI
# placeholders for swan credentials
swan:
  secrets:
    ingress:
      cert:
      key:

